{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Conforme observado no material de backpropagation, o derivado do sigmóide maximiza-se para 0,25. Isso significa que quando você está realizando backpropagation com unidades sigmóides, os erros voltando para a rede será encolhido pelo menos 75% em cada camada. Para camadas próximas à camada de entrada, as atualizações de peso serão pequenas se você tiver um monte de camadas e os pesos levará muito tempo para treinar. Devido a isso, sigmóides têm caído fora de favor como ativações em unidades ocultas.\n",
    "\n",
    "### Rectified Linear Units\n",
    "\n",
    "Em vez de sigmóides, as redes de aprendizagem profunda mais recentes utilizam unidades rectificadas lineares (ReLUs) para as camadas ocultas. Uma unidade linear rectificada tem a saída 0 se a entrada for menor que 0 e a saída bruta caso contrário. Ou seja, se a entrada for maior que 0, a saída é igual à entrada\n",
    "\n",
    "As ativações de ReLU são a função de ativação não-linear mais simples que você pode usar. Quando a entrada é positiva, a derivada é 1, então não há o efeito de fuga que você vê nos erros retropropagados dos sigmóides. A pesquisa mostrou que ReLUs resulta em um treinamento muito mais rápido para grandes redes. A maioria das estruturas como TensorFlow e TFLearn tornam simples usar ReLUs nas camadas ocultas, então você não precisará implementá-las você mesmo.\n",
    "\n",
    "**Desvantagem: ** É possível que um grande gradiente pode definir os pesos de modo que uma unidade de ReLU será sempre 0. Essas unidades \"mortas\" sempre serão 0 e uma grande quantidade de computação será desperdiçada no treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Para problemas de classificação em mais de 2 grupos usa-se a função softmax. A função softmax ajusta as saídas de cada unidade para ser entre 0 e 1, assim como um sigmóide. Também divide cada saída de modo que a soma total das saídas é igual a 1. A saída da função softmax é equivalente a uma distribuição de probabilidade categórica, ela informa a probabilidade de que qualquer uma das classes seja verdadeira.\n",
    "\n",
    "Matematicamente a função softmax é mostrada abaixo, onde z é um vetor das entradas para a camada de saída (se você tem 10 unidades de saída, então há 10 elementos em z). E novamente, j indexa as unidades de saída.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma (z)_j = \\frac {e^{z_j}}{\\sum_{k=1} e^{z_j}} for (j = 1,...,K)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy\n",
    "\n",
    "Usamos o SSE (Sum Squared Error) para medir o erro quando temos apenas um valor de saída. Ao usar o Softmax, nossa saída passa a ser um vetor, e o SSE não é mais aplicável. Neste caso, usamos o CCE.\n",
    "\n",
    "Queremos que o nosso erro seja proporcional à distância entre esses vetores. Para calcular essa distância, usaremos a entropia cruzada. Então, nosso objetivo ao treinar a rede é fazer com que nossos vetores de predição sejam tão próximos quanto possível dos vetores rótulos, minimizando a entropia cruzada.\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589b18f5_cross-entropy-diagram/cross-entropy-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver acima, a entropia cruzada é a soma dos elementos de rótulo vezes o log natural das probabilidades de predição. Note que esta fórmula não é simétrica!\n",
    "\n",
    "O que é legal sobre o uso do *one-hot encoding* para o vetor rótulo é que $y_j$ é 0, exceto para a única classe verdadeira. Então, todos os termos dessa soma, exceto onde $y_j = 1$ são zero e a entropia cruzada (cross-entropy) é $D = - ln ŷ$ para o valor verdadeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
