{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de hyperparameters:\n",
    "\n",
    "- Optimizer hyperparameters: relacionado a otimização e treino. Ex: learning rate, batch size, epochs\n",
    "\n",
    "- Model hyperparamenters: relacionados a estrutura do modelo. Ex: número de camadas, arquitetura do model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate: O mais importante.** É o fator multiplicador que ajusta os pesos a cada etapa de treinamento. Se muito pequeno, demanda muito treinamento. Se muito grande, afasta os pesos do valor ideal.\n",
    "\n",
    "Devido a essa dificuldade em encontrar o valor correto, foram criados algoritmos para ajustar dinamicamente o Learning Rate:\n",
    "\n",
    "- Learning Rate\n",
    "    - [Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay) in TensorFlow.\n",
    "\n",
    "- Adaptive Learning Optimizers (subtrai ou soma, conforme a necessidade)\n",
    "    - [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "    - [AdagradOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minibatch**: Tamanho do pacote de treinamento usado para o foward e backward pass. Pode ser usado apenas um exemplar (online/sthocastic training), todos os dados (batch training) ou uma parte deles (minibatch).\n",
    "\n",
    "O recomendado é usar valores entre 1 e 256 (dobrando os valores). 32 até 256 são bons candidatos.\n",
    "\n",
    "Valores baixos para o minibatch podem tornar o treinamento muito lento. Valores muito altos podem ser mais rápidos, mas podem \"impedir\" o modelo de aprimorar, deixando-o preso em um valor minimo local (ao invez de um valor minimo global)\n",
    "\n",
    "![](bla.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Epochs\n",
    "\n",
    "### SessionRunHook \n",
    "\n",
    "Versões mais recentes do TensorFlow depreciaram monitores em favor de SessionRunHooks. SessionRunHooks são uma parte em evolução do tf.train, e no futuro parecem ser o lugar apropriado onde você implementaria a parada precoce. No momento da redação, dois monitores de parada pré-definidos existem como parte dos ganchos de treinamento do tf.train: \n",
    "\n",
    "- [StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): Um monitor para solicitar a parada de treinamento após um certo número de etapas\n",
    "- [NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): um monitor monitora o loss e para de treinar se encontrar uma perda de NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Units / Layers\n",
    "\n",
    "Redes com muita capacidade (muitas camadas/unidades) tendem ao overfitting. O overfitting se manifesta com uma taxa de acerto maior no treinamento do que na validação. Neste caso pode-se reduzir a quantidade de unidades ou usar dropout ou L2.\n",
    "\n",
    "De modo geral, quanto mais unidades (hidden units) melhor, mas unidades em muito excesso podem levar ao overfitting.\n",
    "\n",
    "Se o modelo não aprender, adicione mais unidades e pare qndo a taxa de acerto da validação cair.\n",
    "\n",
    "A primeira camada oculta (hidden layer) deve ser maior (mais unidades) que a camada de input para a maioria dos casos.\n",
    "\n",
    "Sobre a quantidade de camadas, geralmente 3 camadas supera 2 camadas, mas aprofundar muito mais que isso não costuma dar resultado. Com exceção das CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers).\" ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "Mais sobre capacidade em: [Deep Learning book, chapter 5.2](http://www.deeplearningbook.org/contents/ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Hyperparameters\n",
    "\n",
    "LSTM e GRU celulas performam melhor que Vanilla RNN na maioria dos casos.\n",
    "\n",
    "Entre LSTM e GRU a recomendação é testar os 2 com um subset de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM Vs GRU\n",
    "\"These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task.\"\n",
    "\n",
    "[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio\n",
    "\n",
    "\"The GRU outperformed the LSTM on all tasks with the exception of language modelling\"\n",
    "\n",
    "[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever\n",
    "\n",
    "\"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN.\"\n",
    "\n",
    "[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei\n",
    "\n",
    "\"Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.\"\n",
    "\n",
    "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah\n",
    "\n",
    "\"In our [Neural Machine Translation] experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells. Somewhat to our surprise, we found that the vanilla decoder is unable to learn nearly as well as the gated variant.\"\n",
    "\n",
    "[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906v2) by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RNN Archit.png)\n",
    "\n",
    "1 - https://arxiv.org/abs/1610.09975\n",
    "\n",
    "2 - https://arxiv.org/abs/1303.5778\n",
    "\n",
    "3 - https://arxiv.org/abs/1409.3215\n",
    "\n",
    "4 - https://arxiv.org/abs/1411.4555\n",
    "\n",
    "5 - https://arxiv.org/abs/1502.04623\n",
    "\n",
    "6 - http://www.aclweb.org/anthology/P15-2116\n",
    "\n",
    "7 - https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about hyperparameters, these are some great resources on the topic:\n",
    "\n",
    "- [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533) by Yoshua Bengio\n",
    "- [Deep Learning book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html) by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n",
    "- [Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters) by Michael Nielsen\n",
    "- [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) (pdf) by Yann LeCun\n",
    "\n",
    "\n",
    "More specialized sources:\n",
    "\n",
    "- [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523) by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao\n",
    "- [Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas\n",
    "- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
