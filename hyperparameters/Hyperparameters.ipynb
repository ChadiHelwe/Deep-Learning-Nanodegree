{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de hyperparameters:\n",
    "\n",
    "- Optimizer hyperparameters: relacionado a otimização e treino. Ex: learning rate, batch size, epochs\n",
    "\n",
    "- Model hyperparamenters: relacionados a estrutura do modelo. Ex: número de camadas, arquitetura do model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate: O mais importante.** É o fator multiplicador que ajusta os pesos a cada etapa de treinamento. Se muito pequeno, demanda muito treinamento. Se muito grande, afasta os pesos do valor ideal.\n",
    "\n",
    "Devido a essa dificuldade em encontrar o valor correto, foram criados algoritmos para ajustar dinamicamente o Learning Rate:\n",
    "\n",
    "- Learning Rate\n",
    "    - [Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay) in TensorFlow.\n",
    "\n",
    "- Adaptive Learning Optimizers (subtrai ou soma, conforme a necessidade)\n",
    "    - [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "    - [AdagradOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minibatch**: Tamanho do pacote de treinamento usado para o foward e backward pass. Pode ser usado apenas um exemplar (online/sthocastic training), todos os dados (batch training) ou uma parte deles (minibatch).\n",
    "\n",
    "O recomendado é usar valores entre 1 e 256 (dobrando os valores). 32 até 256 são bons candidatos.\n",
    "\n",
    "Valores baixos para o minibatch podem tornar o treinamento muito lento. Valores muito altos podem ser mais rápidos, mas podem \"impedir\" o modelo de aprimorar, deixando-o preso em um valor minimo local (ao invez de um valor minimo global)\n",
    "\n",
    "![](bla.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Epochs\n",
    "\n",
    "### SessionRunHook \n",
    "\n",
    "Versões mais recentes do TensorFlow depreciaram monitores em favor de SessionRunHooks. SessionRunHooks são uma parte em evolução do tf.train, e no futuro parecem ser o lugar apropriado onde você implementaria a parada precoce. No momento da redação, dois monitores de parada pré-definidos existem como parte dos ganchos de treinamento do tf.train: \n",
    "\n",
    "- [StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): Um monitor para solicitar a parada de treinamento após um certo número de etapas\n",
    "- [NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): um monitor monitora o loss e para de treinar se encontrar uma perda de NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Units / Layers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
